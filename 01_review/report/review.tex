\documentclass[letterpaper,11pt]{article}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=0.75in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=black,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
\usepackage{blindtext}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% MY PACKAGES

% smart par skip
\usepackage{parskip}

% used to create cute quote 
\usepackage{csquotes}

% fix annoying positioning
\usepackage{float}

% cute tables
\usepackage{booktabs}

% testo intorno a figure
\usepackage{wrapfig, blindtext}

% used to highlight stuff
\usepackage{color,soul}

% MORE MATH
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}

% line breaks in cells
\usepackage{makecell, boldline}

% fix urls in bib
\usepackage{etoolbox}
\appto\UrlBreaks{\do\-}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CUSTOM SETTINGS

\setlength\parindent{0pt}

% IMAGES SHORTCUT
\graphicspath{ {./images/} }

% INLINE CODE TEXT
\definecolor{codegray}{gray}{0.9}
\newcommand{\code}[1]{\colorbox{codegray}{\texttt{#1}}}

% CUSTOM LINESPREAD
\linespread{1.05}

% FAST CENTERED IMAGE
\newcommand{\imgc}[3]{\begin{figure}[H] 
  \centering
  \includegraphics[width=#1]{#2}
  \caption{#3}
\end{figure}}

% FIX CAPTION SPACE
\usepackage[font=small,skip=0.25cm]{caption}
\setlength{\belowcaptionskip}{-0.25cm}

% custom line width
\makeatletter
\def\thickhline{%
  \noalign{\ifnum0=`}\fi\hrule \@height \thickarrayrulewidth \futurelet
   \reserved@a\@xthickhline}
\def\@xthickhline{\ifx\reserved@a\thickhline
               \vskip\doublerulesep
               \vskip-\thickarrayrulewidth
             \fi
      \ifnum0=`{\fi}}
\makeatother

% custom citations
\usepackage{cite}

% images wrappers

\usepackage{graphicx}
\usepackage{wrapfig}

% page background color
\usepackage{xcolor}
\definecolor{resblack}{RGB}{30,30,30}
\definecolor{ivory}{RGB}{244,239,233}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CUSTOM PYTHON CODE

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagecolor{ivory}

\title{\textbf{Review}}
\author{\textbf{Introduction to ML for Natural Language Processing}\\ \\ Daniele Passab√¨ [221229], Data Science\\}

\date{\textit{\\Making the V in VQA Matter:\\Elevating the Role of Image Understanding in Visual Question Answering}}
\maketitle
\thispagestyle{empty}

\begin{figure}[H] 
  \centering
  \includegraphics[width=4cm]{logo.png}
\end{figure}

% \newpage
% \pagecolor{white}
% \setcounter{tocdepth}{2}
% \tableofcontents
% \thispagestyle{empty}

\newpage
\clearpage
\pagecolor{white}
\pagenumbering{arabic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START HERE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%
% SUMMARY %
%%%%%%%%%%%

\section{Summary}

The paper \cite{goyal2017making} is concerned with \textit{Visual Question Answering} (VQA), a task in computer vision in which a system receives a textual question about an image and has to deduce the answer \cite{kafle2017visual}. According to \textit{Goyal et al.}, the inherent structure of the world we live in and the biases inevitably present in our language are signals that are most easily grasped by machine learning models, which mostly ignore the visual aspect of the data. Using a dataset not constructed keeping these considerations in mind leads to a distorted conception of the real capabilities of the models, which are overestimated.

The authors do something that in our opinion research should always do: question itself in order to improve. They expand the dataset proposed by \textit{Antol et al.} \cite{antol2015vqa}, seeking to improve its balance. They then employ the new dataset to test several state-of-the-art VQA models. The results obtained on this balanced dataset are significantly worse than those obtained on the original one. This empirical evidence shows that the models were exploiting language priors, instead of using the combined information of text and image. 

Furthermore, the authors develop a new interpretable architecture: given an input (image, question) the model provides a counter-explanation based on examples. This means that in addition to the answer to the question asked, an image is also returned as output which the model considers visually similar to the one taken as input, but that has a different answer to the question asked.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% STRENGTH AND WEAKNESSES %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Strength and weaknesses}
\label{sec:procons}

One of the most striking and convincing points of this article is the way the authors justify every choice behind their actions: this gives the impression that nothing is left to chance. Furthermore, with the exception of a few minor repetitions, the text was written in an extremely clear and concise manner.

Although it would have been more than sufficient, the authors did not stop at augmenting the dataset and make use of existing architectures. In fact, they even created their own model. An implementation of this kind demonstrates in our opinion a real and deep knowledge of models and architectures, increasing the reader's confidence in the researchers.

It is not obvious to find weaknesses in this paper, which we consider to be very precise and of high quality. The methodology followed by the authors is very robust and will be discussed in detail in section \ref{sec:soundness}. However, one choice we could not help but question is the use of Amazon Mechanical Turk (AMT), employed during the balancing phase of the dataset by the authors, who outsourced the task. Platforms such as AMT are often in the spotlight for ethical and moral issues \cite{fort2011amazon}. The so-called \textit{workers} perform extremely repetitive tasks for very little money and do not benefit from any of the conventional workers' rights. In fact, the platform itself is often labelled a \textit{digital sweatshop}. The authors of the paper do not specify what salary was provided to each worker, which might suggest an unethical use of the already ambiguous platform.


%%%%%%%%%%%%%%%%%%%%
% POTENTIAL IMPACT %
%%%%%%%%%%%%%%%%%%%%

\section{Potential Impact}

The authors of the article started from an intuition and, thanks to their extensive research, were able to demonstrate for the first time how the models considered to be state-of-the-art in the field of VQA were exploiting the biases present in language. This could potentially be a major milestone from which other researchers can build on and extend the work already accomplished.

An indication of how much the work of \textit{Goyal et al.} was appreciated and acknowledged may be inferred by using Google Scholar \cite{google_scholar}, a free search engine that indexes the full text and metadata of scientific literature. By searching within the platform for the keyword \textit{``VQA''}, the article we are discussing is the second result (out of about 20 thousands possible papers). Furthermore, at the time of the writing of this review, the paper has more than 1300 citations.


%%%%%%%%%%%%%%%%%%%%%%%%%
% SOUNDNESS OF THE WORK %
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Soundness of the Work}
\label{sec:soundness}

We find the methodology followed by the authors of the paper to be nearly flawless: they perceived a problem, questioned the current results obtained in the literature, asked interesting and complex questions that needed to be answered, and finally proceeded in the best possible way to address these questions. 

Firstly, the motivation behind the enlargement of the initial dataset is very valid. The latter, in fact, presented strong imbalances. For example, as is mentioned in the paper, the answer to questions starting with \textit{``Do you see a''} was yes in 87\% of the cases. The authors' data collection work was commendable: they managed to almost double the size of the original dataset, which was already considerable, obtaining a total of 1.1 million pairs (image, question). To achieve this, they used Amazon Mechanical Turk, a crowdsourcing platform that allowed them to outsource the image selection process. Despite the misgivings we expressed in section \ref{sec:procons}, AMT is a tool that enables valid results, thanks to the possibility of pre-selection of workers.

Testing pre-existing state of the art models in the literature represents another very logical step in the authors' methodology. This choice, along with the one of using a baseline model that does not employ images to make predictions allowed them to empirically demonstrate the presence of language priors and bias in the models, and the extent to which they were not making full use of the information contained in the images.

The models were trained and tested several times, on different combinations of datasets:
\begin{itemize}
  \itemsep-0.25em
  \item $UU$ \verb|    | trained on Unbalanced dataset, tested on Unbalanced dataset
  \item $UB$ \verb|    | trained on Unbalanced dataset, tested on Balanced dataset
  \item $B_{half}B$ \verb| | trained on half Balanced dataset, tested on Balanced dataset
  \item $BB$ \verb|    | trained on Balanced dataset, tested on Balanced dataset
\end{itemize}

This train/test combination chosen by the authors provides a comprehensive answer to the research question proposed in the article. From the results, one can observe not only how VQA models trained on the unbalanced set decrease in accuracy when tested on the balanced dataset, but also how training performed on a balanced dataset (whether $B_{half}$ or $B$) benefits the performance of the architectures. Out of sheer curiosity, we would be intrigued to have the performance of models trained on $B_{half}$/$B$ and tested on $U$.

Focusing further on the performances of the models, we find the distinction in terms of accuracy based on the type of response the model was intended to give quite logical. In this manner, the authors highlight how the binary case is yet more prone to language bias. The authors' approach hints at how separating the types of possible model responses can simplify the distinction of truly performing models from those that are merely exploiting language priors. 

We conclude this section by discussing the novel model capable of creating counter-examples, which is very difficult to evaluate given the inherent nature of the problem. The authors could have decided the \textit{``rules of the game''} in their favour but, although their results are the highest, they seemed to us to be as unbiased and objective as possible.


%%%%%%%%%%%%%%%%%
% REPLICABILITY %
%%%%%%%%%%%%%%%%%

\section{Replicability}
\label{sec:replicability}

Thanks to the very detailed explanation on the creation of the balanced dataset using AMT, it would be technically possible for an interested researcher to recreate the data augmentation procedure. Naturally, obtaining exactly the same results as the authors is statistically unlikely, which is why we find it very positive that the final balanced dataset has been made public. This not only allows everyone to assess its quality, but also enables it to be employed for further experiments which could lead to advances in the field of VQA. 

The tests performed on the models should also be technically straightforward to reproduce, as the architectures used by the authors are public.

Although it can by no means be said that the explanations concerning the model implemented by the researchers are lacking in clarity, when discussing such complex architectures, it is in our opinion preferable to have the code available. Without it, trying to reproduce exactly the same architecture, even if only to verify the same thesis demonstrated by the authors, becomes very complex.

We firmly believe that the world of research should be more balanced between \textit{competition} and \textit{cooperation}. After all, it would not have been possible to write this article without the state-of-the-art VQA models made public, or without the open-source dataset from which the authors started to create their balanced dataset. 

As a final note, we do understand that the paper aims to be very brief and concise, but we would have welcomed some more technical information. For example, what are the hyperparameters chosen for the convolutional networks or LSTMs in the model implemented by the authors? What framework was used to develop the architecture? What means are needed to be able to train models on such a large number of images?

%%%%%%%%%%%%%
% SUBSTANCE %
%%%%%%%%%%%%%

\section{Substance}

Notwithstanding the fact that the paper was the result of a very extensive work by the authors, there are always means by which one can broaden the work undertaken. Some proposals follow in this section.

In section \textit{4. Benchmarking Existing VQA Models}, the authors argue that more pairs (image, question) could benefit the performance of the models, that could be data starved. This is supported by showing that models trained on $B$ were more accurate than those trained on $B_{half}$. To further confirm this hypothesis, it would be interesting to train the models on progressively fewer observations. Maintaining the authors' logic and notation, the datasets could be $B_{\frac{1}{4}}$, $B_{\frac{1}{8}}$, etc. It would then become possible to show empirically how abundant data benefits VQA architectures.

Concurrently, we believe that it would be of interest to train the models on a fully balanced dataset, even if this means fewer pairs (image, question). This dataset, which we will call $B_{perfectly\_balanced}$, should have the same probability of answering every possible question. For example: if the question is \textit{``Which is the sport being played in the picture?''} the possible answers should be tennis (25\%), football (25\%), swimming (25\%), baseball (25\%).

These approaches would enable to study the trade-off between \textit{balance} and \textit{data abundance}, understanding which of the two aspects is more predominant in determining the architectures' performance.

%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY %
%%%%%%%%%%%%%%%%

% add bib to TOC
\addcontentsline{toc}{section}{References}

\newpage
\bibliographystyle{plain}
\bibliography{biblio}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
