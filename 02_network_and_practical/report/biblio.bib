
% INTRODUCTION

@article{nadkarni2011natural,
  title     = {Natural language processing: an introduction},
  author    = {Nadkarni, Prakash M and Ohno-Machado, Lucila and Chapman, Wendy W},
  journal   = {Journal of the American Medical Informatics Association},
  volume    = {18},
  number    = {5},
  pages     = {544--551},
  year      = {2011},
  publisher = {Oxford Academic}
}

@misc{IBM_NLP,
  author       = {IBM},
  title        = {{Natural Language Processing (NLP)}},
  howpublished = {\\ \url{https://www.ibm.com/cloud/learn/natural-language-processing}},
  note         = {\\Accessed: 21-05-2022}
}

@misc{TDS_NLP,
  author       = {Towards Data Science},
  title        = {{The Importance of Natural Language Processing for Non-English Languages}},
  howpublished = {\\ \url{https://towardsdatascience.com/the-importance-of-natural-language-processing-for-non-english-languages-ada463697b9d}},
  note         = {\\Accessed: 21-05-2022}
}

% DATASET

@inproceedings{Chalkidis2021MultiEURLEXA,
  title     = {{MultiEURLEX - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer}},
  author    = {Ilias Chalkidis and Manos Fergadiotis and Ion Androutsopoulos},
  booktitle = {EMNLP},
  year      = {2021}
}


% ARCHITECTURES

% Long Short Term Memory

@misc{LSTM_gentle_intro,
  author       = {Jason Brownlee},
  title        = {{A Gentle Introduction to Long Short-Term Memory Networks by the Experts}},
  howpublished = {\\ \url{https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/}},
  note         = {\\Accessed: 23-05-2022}
}

@article{Graves2005FramewisePC,
  title   = {Framewise phoneme classification with bidirectional LSTM networks},
  author  = {Alex Graves and Juergen Schmidhuber},
  journal = {Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},
  year    = {2005},
  volume  = {4},
  pages   = {2047-2052 vol. 4}
}

@article{Zaremba2014RecurrentNN,
  title   = {Recurrent Neural Network Regularization},
  author  = {Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
  journal = {ArXiv},
  year    = {2014},
  volume  = {abs/1409.2329}
}

% Convolutional Neural Network

@article{Albawi2017UnderstandingOA,
  title   = {Understanding of a convolutional neural network},
  author  = {Saad Albawi and Tareq Abed Mohammed and Saad Al-Zawi},
  journal = {2017 International Conference on Engineering and Technology (ICET)},
  year    = {2017},
  pages   = {1-6}
}

@misc{CNN_how,
  author       = {Jason Brownlee},
  title        = {{How Do Convolutional Layers Work in Deep Learning Neural Networks?}},
  howpublished = {\\ \url{https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/}},
  note         = {\\Accessed: 27-05-2022}
}

@misc{MaxPool_how,
  author       = {Jason Brownlee},
  title        = {{A Gentle Introduction to Pooling Layers for Convolutional Neural Networks}},
  howpublished = {\\ \url{https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/}},
  note         = {\\Accessed: 27-05-2022}
}





% EXPERIMENTAL SETUP

% Preprocessing

@article{lovins1968development,
  title   = {Development of a stemming algorithm.},
  author  = {Lovins, Julie Beth},
  journal = {Mech. Transl. Comput. Linguistics},
  volume  = {11},
  number  = {1-2},
  pages   = {22--31},
  year    = {1968}
}

@article{balakrishnan2014stemming,
  title  = {Stemming and lemmatization: a comparison of retrieval performances},
  author = {Balakrishnan, Vimala and Lloyd-Yemoh, Ethel},
  year   = {2014}
}

% PYTORCH

@misc{pytorch_embedding,
  author       = {PyTorch},
  title        = {{Embedding}},
  howpublished = {\\ \url{https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html}},
  note         = {\\Accessed: 24-05-2022}
}

@misc{pytorch_lstm,
  author       = {PyTorch},
  title        = {{LSTM}},
  howpublished = {\\ \url{https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html}},
  note         = {\\Accessed: 24-05-2022}
}

@misc{pytorch_linear,
  author       = {PyTorch},
  title        = {{Linear}},
  howpublished = {\\ \url{https://pytorch.org/docs/stable/generated/torch.nn.Linear.html}},
  note         = {\\Accessed: 24-05-2022}
}

@misc{pytorch_dropout,
  author       = {PyTorch},
  title        = {{Dropout}},
  howpublished = {\\ \url{https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html}},
  note         = {\\Accessed: 24-05-2022}
}

% SKLEARN

@article{scikit-learn,
  title   = {Scikit-learn: Machine Learning in {P}ython},
  author  = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
             and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
             and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  pages   = {2825--2830},
  year    = {2011}
}

% LOSS

@misc{cross_entropy,
  author       = {Jason Brownlee},
  title        = {{A Gentle Introduction to Cross-Entropy for Machine Learning}},
  howpublished = {\\ \url{https://machinelearningmastery.com/cross-entropy-for-machine-learning/}},
  note         = {\\Accessed: 27-05-2022}
}

% RESULTS

@article{cahyani2021performance,
  title={Performance comparison of TF-IDF and Word2Vec models for emotion text classification},
  author={Cahyani, Denis Eka and Patasik, Irene},
  journal={Bulletin of Electrical Engineering and Informatics},
  volume={10},
  number={5},
  pages={2780--2788},
  year={2021}
}

% TUNER

@misc{omalley2019kerastuner,
    title        = {KerasTuner},
    author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},
    year         = 2019,
    howpublished = {\url{https://github.com/keras-team/keras-tuner}}
}