{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLP Practical**\n",
    "\n",
    "### **LSTM**\n",
    "\n",
    "It is now time to train a Recurrent Neural Nework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries**\n",
    "\n",
    "We import the necessary libraries for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Libraries Imported\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"> Libraries Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import the dataset**\n",
    "\n",
    "We read our pickle with the encoded texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>celex_id</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_new</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_de</th>\n",
       "      <th>text_it</th>\n",
       "      <th>text_pl</th>\n",
       "      <th>text_sv</th>\n",
       "      <th>text_en_enc</th>\n",
       "      <th>text_de_enc</th>\n",
       "      <th>text_it_enc</th>\n",
       "      <th>text_pl_enc</th>\n",
       "      <th>text_sv_enc</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32006D0213</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>commission decision of _number_ march _number_...</td>\n",
       "      <td>entscheidung der kommission vom _number_ marz ...</td>\n",
       "      <td>decisione della commissione del _number_ marzo...</td>\n",
       "      <td>decyzja komisji z dnia _number_ marca _number_...</td>\n",
       "      <td>kommissionens beslut av den _number_ mars _num...</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 5, 7, 8, 9, 4, 10, 11, 12, 13...</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 6, 8, 9, 3, 10, 11, 12, 13...</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 6, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 6, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>[[2, 3, 4, 5, 6, 7, 6, 8, 9, 10, 11, 12, 13, 1...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32003R1786</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>council regulation ec no _number_ _number_ of ...</td>\n",
       "      <td>verordnung eg nr _number_ _number_ des rates v...</td>\n",
       "      <td>regolamento ce n _number_ _number_ del consigl...</td>\n",
       "      <td>rozporzadzenie rady we nr _number_ _number_ z ...</td>\n",
       "      <td>radets forordning eg nr _number_ _number_ av d...</td>\n",
       "      <td>[[133, 177, 34, 92, 5, 5, 4, 5, 178, 5, 44, 8,...</td>\n",
       "      <td>[[196, 31, 98, 6, 6, 42, 43, 5, 6, 197, 6, 49,...</td>\n",
       "      <td>[[219, 41, 110, 6, 6, 5, 53, 5, 6, 220, 6, 221...</td>\n",
       "      <td>[[242, 49, 39, 32, 6, 6, 4, 5, 6, 243, 6, 8, 1...</td>\n",
       "      <td>[[49, 189, 38, 33, 6, 6, 4, 5, 6, 190, 6, 8, 5...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32004R1038</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>commission regulation ec no _number_ _number_ ...</td>\n",
       "      <td>verordnung eg nr _number_ _number_ der kommiss...</td>\n",
       "      <td>regolamento ce n _number_ _number_ della commi...</td>\n",
       "      <td>rozporzadzenie komisji we nr _number_ _number_...</td>\n",
       "      <td>kommissionens forordning eg nr _number_ _numbe...</td>\n",
       "      <td>[[2, 177, 34, 92, 5, 5, 4, 5, 74, 5, 665, 8, 2...</td>\n",
       "      <td>[[196, 31, 98, 6, 6, 3, 4, 5, 6, 454, 6, 8, 78...</td>\n",
       "      <td>[[219, 41, 110, 6, 6, 3, 4, 5, 6, 471, 6, 8, 8...</td>\n",
       "      <td>[[242, 3, 39, 32, 6, 6, 4, 5, 6, 552, 6, 8, 10...</td>\n",
       "      <td>[[2, 189, 38, 33, 6, 6, 4, 5, 6, 442, 6, 8, 72...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32003R1012</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>commission regulation ec no _number_ _number_ ...</td>\n",
       "      <td>verordnung eg nr _number_ _number_ der kommiss...</td>\n",
       "      <td>regolamento ce n _number_ _number_ della commi...</td>\n",
       "      <td>rozporzadzenie komisji we nr _number_ _number_...</td>\n",
       "      <td>kommissionens forordning eg nr _number_ _numbe...</td>\n",
       "      <td>[[2, 177, 34, 92, 5, 5, 4, 5, 352, 5, 688, 14,...</td>\n",
       "      <td>[[196, 31, 98, 6, 6, 3, 4, 5, 6, 402, 6, 8, 80...</td>\n",
       "      <td>[[219, 41, 110, 6, 6, 3, 4, 5, 6, 417, 6, 418,...</td>\n",
       "      <td>[[242, 3, 39, 32, 6, 6, 4, 5, 6, 497, 6, 8, 11...</td>\n",
       "      <td>[[2, 189, 38, 33, 6, 6, 4, 5, 6, 397, 6, 8, 77...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32003R2229</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>council regulation ec no _number_ _number_ of ...</td>\n",
       "      <td>verordnung eg nr _number_ _number_ des rates v...</td>\n",
       "      <td>regolamento ce n _number_ _number_ del consigl...</td>\n",
       "      <td>rozporzadzenie rady we nr _number_ _number_ z ...</td>\n",
       "      <td>radets forordning eg nr _number_ _number_ av d...</td>\n",
       "      <td>[[133, 177, 34, 92, 5, 5, 4, 5, 43, 5, 690, 94...</td>\n",
       "      <td>[[196, 31, 98, 6, 6, 42, 43, 5, 6, 44, 6, 8, 8...</td>\n",
       "      <td>[[219, 41, 110, 6, 6, 5, 53, 5, 6, 54, 6, 8, 4...</td>\n",
       "      <td>[[242, 49, 39, 32, 6, 6, 4, 5, 6, 51, 6, 8, 11...</td>\n",
       "      <td>[[49, 189, 38, 33, 6, 6, 4, 5, 6, 52, 6, 8, 78...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     celex_id  labels  labels_new  \\\n",
       "0  32006D0213       1           1   \n",
       "1  32003R1786       3           3   \n",
       "2  32004R1038       3           3   \n",
       "3  32003R1012       2           2   \n",
       "4  32003R2229      18          11   \n",
       "\n",
       "                                             text_en  \\\n",
       "0  commission decision of _number_ march _number_...   \n",
       "1  council regulation ec no _number_ _number_ of ...   \n",
       "2  commission regulation ec no _number_ _number_ ...   \n",
       "3  commission regulation ec no _number_ _number_ ...   \n",
       "4  council regulation ec no _number_ _number_ of ...   \n",
       "\n",
       "                                             text_de  \\\n",
       "0  entscheidung der kommission vom _number_ marz ...   \n",
       "1  verordnung eg nr _number_ _number_ des rates v...   \n",
       "2  verordnung eg nr _number_ _number_ der kommiss...   \n",
       "3  verordnung eg nr _number_ _number_ der kommiss...   \n",
       "4  verordnung eg nr _number_ _number_ des rates v...   \n",
       "\n",
       "                                             text_it  \\\n",
       "0  decisione della commissione del _number_ marzo...   \n",
       "1  regolamento ce n _number_ _number_ del consigl...   \n",
       "2  regolamento ce n _number_ _number_ della commi...   \n",
       "3  regolamento ce n _number_ _number_ della commi...   \n",
       "4  regolamento ce n _number_ _number_ del consigl...   \n",
       "\n",
       "                                             text_pl  \\\n",
       "0  decyzja komisji z dnia _number_ marca _number_...   \n",
       "1  rozporzadzenie rady we nr _number_ _number_ z ...   \n",
       "2  rozporzadzenie komisji we nr _number_ _number_...   \n",
       "3  rozporzadzenie komisji we nr _number_ _number_...   \n",
       "4  rozporzadzenie rady we nr _number_ _number_ z ...   \n",
       "\n",
       "                                             text_sv  \\\n",
       "0  kommissionens beslut av den _number_ mars _num...   \n",
       "1  radets forordning eg nr _number_ _number_ av d...   \n",
       "2  kommissionens forordning eg nr _number_ _numbe...   \n",
       "3  kommissionens forordning eg nr _number_ _numbe...   \n",
       "4  radets forordning eg nr _number_ _number_ av d...   \n",
       "\n",
       "                                         text_en_enc  \\\n",
       "0  [[2, 3, 4, 5, 6, 5, 7, 8, 9, 4, 10, 11, 12, 13...   \n",
       "1  [[133, 177, 34, 92, 5, 5, 4, 5, 178, 5, 44, 8,...   \n",
       "2  [[2, 177, 34, 92, 5, 5, 4, 5, 74, 5, 665, 8, 2...   \n",
       "3  [[2, 177, 34, 92, 5, 5, 4, 5, 352, 5, 688, 14,...   \n",
       "4  [[133, 177, 34, 92, 5, 5, 4, 5, 43, 5, 690, 94...   \n",
       "\n",
       "                                         text_de_enc  \\\n",
       "0  [[2, 3, 4, 5, 6, 7, 6, 8, 9, 3, 10, 11, 12, 13...   \n",
       "1  [[196, 31, 98, 6, 6, 42, 43, 5, 6, 197, 6, 49,...   \n",
       "2  [[196, 31, 98, 6, 6, 3, 4, 5, 6, 454, 6, 8, 78...   \n",
       "3  [[196, 31, 98, 6, 6, 3, 4, 5, 6, 402, 6, 8, 80...   \n",
       "4  [[196, 31, 98, 6, 6, 42, 43, 5, 6, 44, 6, 8, 8...   \n",
       "\n",
       "                                         text_it_enc  \\\n",
       "0  [[2, 3, 4, 5, 6, 7, 6, 8, 9, 10, 11, 12, 13, 1...   \n",
       "1  [[219, 41, 110, 6, 6, 5, 53, 5, 6, 220, 6, 221...   \n",
       "2  [[219, 41, 110, 6, 6, 3, 4, 5, 6, 471, 6, 8, 8...   \n",
       "3  [[219, 41, 110, 6, 6, 3, 4, 5, 6, 417, 6, 418,...   \n",
       "4  [[219, 41, 110, 6, 6, 5, 53, 5, 6, 54, 6, 8, 4...   \n",
       "\n",
       "                                         text_pl_enc  \\\n",
       "0  [[2, 3, 4, 5, 6, 7, 6, 8, 9, 10, 11, 12, 13, 1...   \n",
       "1  [[242, 49, 39, 32, 6, 6, 4, 5, 6, 243, 6, 8, 1...   \n",
       "2  [[242, 3, 39, 32, 6, 6, 4, 5, 6, 552, 6, 8, 10...   \n",
       "3  [[242, 3, 39, 32, 6, 6, 4, 5, 6, 497, 6, 8, 11...   \n",
       "4  [[242, 49, 39, 32, 6, 6, 4, 5, 6, 51, 6, 8, 11...   \n",
       "\n",
       "                                         text_sv_enc    set  \n",
       "0  [[2, 3, 4, 5, 6, 7, 6, 8, 9, 10, 11, 12, 13, 1...  train  \n",
       "1  [[49, 189, 38, 33, 6, 6, 4, 5, 6, 190, 6, 8, 5...  train  \n",
       "2  [[2, 189, 38, 33, 6, 6, 4, 5, 6, 442, 6, 8, 72...  train  \n",
       "3  [[2, 189, 38, 33, 6, 6, 4, 5, 6, 397, 6, 8, 77...  train  \n",
       "4  [[49, 189, 38, 33, 6, 6, 4, 5, 6, 52, 6, 8, 78...  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_pickle(\"../data/3_multi_eurlex_encoded.pkl\")\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Get the splits**\n",
    "\n",
    "We already divided the dataset in 3 splits, so we can just use the information in the *set* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "dataframe_train = dataframe.loc[dataframe[\"set\"] == \"train\"]\n",
    "X_train = list(dataframe_train[\"text_en_enc\"])\n",
    "Y_train = list(dataframe_train[\"labels_new\"])\n",
    "\n",
    "# validation \n",
    "dataframe_val = dataframe.loc[dataframe[\"set\"] == \"validation\"]\n",
    "X_val = list(dataframe_val[\"text_en_enc\"])\n",
    "Y_val = list(dataframe_val[\"labels_new\"])\n",
    "\n",
    "# test \n",
    "dataframe_test = dataframe.loc[dataframe[\"set\"] == \"test\"]\n",
    "X_test = list(dataframe_test[\"text_en_enc\"])\n",
    "Y_test = list(dataframe_test[\"labels_new\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_temp = list(dataframe['text_en_enc'])\n",
    "# Y_temp = list(dataframe['labels_new'])\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train_temp, X_valid_temp, y_train_temp, y_valid_temp = train_test_split(X_temp, Y_temp, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUZIONE TEMPORANEA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=1)\n",
    "\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create a Pytorch dataset**\n",
    "\n",
    "We create a custom class that inherits from the pytorch Dataset class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextsDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextsDataset(X_train, Y_train)\n",
    "valid_dataset = TextsDataset(X_val, Y_val)\n",
    "\n",
    "# DEBUG\n",
    "# train_dataset_temp = TextsDataset(X_train_temp, y_train_temp)\n",
    "# valid_dataset_temp = TextsDataset(X_valid_temp, y_valid_temp)\n",
    "# DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPOCHS = 30\n",
    "\n",
    "VOCAB_SIZE = 78549      # number of words in EN_WORDS\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "EMBEDDING_DIM=500\n",
    "HIDDEN_DIM=50\n",
    "NUM_CLASSES=12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dl = DataLoader(train_dataset_temp, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# valid_dl = DataLoader(valid_dataset_temp, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create training function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=10, lr=0.001):\n",
    "\n",
    "    # setup loss and optimizers\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "\n",
    "    # for each epoch...\n",
    "    for i in range(epochs):\n",
    "\n",
    "        print(f\"> EPOCH {i+1} started\")\n",
    "\n",
    "        # training mode\n",
    "        model.train()\n",
    "\n",
    "        # reset loss and total\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        for x, y, l in tqdm(train_dl):\n",
    "\n",
    "            # input preprocess\n",
    "            x = x.long()\n",
    "            y = y.long()\n",
    "\n",
    "            # forward pass\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(y_pred, y)\n",
    "\n",
    "            # backwards pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "\n",
    "        # evaluate the model on the validation set\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, valid_dl)\n",
    "\n",
    "        print(f\"> EPOCH {i+1} completed\")\n",
    "        print(f\" - Training Loss        {sum_loss/total}\")\n",
    "        print(f\" - Validation Loss      {val_loss}\")\n",
    "        print(f\" - Validation Accuracy  {val_acc}\")\n",
    "        print(f\" - Validation RMSE      {val_rmse}\")\n",
    "        print(\"=========================================\")\n",
    "\n",
    "\n",
    "def validation_metrics (model, valid_dl):\n",
    "\n",
    "    # evaluation mode (no need for backwards propagation)\n",
    "    model.eval()\n",
    "\n",
    "    # setup placeholders\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "\n",
    "    for x, y, l in valid_dl:\n",
    "\n",
    "        # input preprocess\n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "\n",
    "        # obtain predictions\n",
    "        y_hat = model(x, l)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1]\n",
    "\n",
    "        # update results\n",
    "        correct += (pred == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n",
    "        \n",
    "    return sum_loss/total, correct/total, sum_rmse/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LSTM with fixed length input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_fixed_len(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "\n",
    "        # calls the constructor of the parent (nn.Module) \n",
    "        # --> so that any initialization done in the super class is still done\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Dropout Layer\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "\n",
    "        # Set initial hidden and cell states \n",
    "        x = self.embeddings(x)\n",
    "        #x = self.dropout(x)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "\n",
    "        # Decode the hidden state of the last time step and return\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_fixed = LSTM_fixed_len(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> EPOCH 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 90/1973 [01:14<25:49,  1.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Daniele\\Documents\\Programmazione\\Github\\Uni_NLP2022_Exam\\03_practical\\src\\05_LSTM.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Daniele/Documents/Programmazione/Github/Uni_NLP2022_Exam/03_practical/src/05_LSTM.ipynb#ch0000051?line=0'>1</a>\u001b[0m train_model(LSTM_fixed, epochs\u001b[39m=\u001b[39;49mEPOCHS, lr\u001b[39m=\u001b[39;49mLEARNING_RATE)\n",
      "\u001b[1;32mc:\\Users\\Daniele\\Documents\\Programmazione\\Github\\Uni_NLP2022_Exam\\03_practical\\src\\05_LSTM.ipynb Cell 20'\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, epochs, lr)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniele/Documents/Programmazione/Github/Uni_NLP2022_Exam/03_practical/src/05_LSTM.ipynb#ch0000037?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(y_pred, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniele/Documents/Programmazione/Github/Uni_NLP2022_Exam/03_practical/src/05_LSTM.ipynb#ch0000037?line=29'>30</a>\u001b[0m \u001b[39m# backwards pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Daniele/Documents/Programmazione/Github/Uni_NLP2022_Exam/03_practical/src/05_LSTM.ipynb#ch0000037?line=30'>31</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniele/Documents/Programmazione/Github/Uni_NLP2022_Exam/03_practical/src/05_LSTM.ipynb#ch0000037?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Daniele/Documents/Programmazione/Github/Uni_NLP2022_Exam/03_practical/src/05_LSTM.ipynb#ch0000037?line=33'>34</a>\u001b[0m sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\u001b[39m*\u001b[39my\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Programmi\\Anaconda\\envs\\NLP_2022\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\Programmi\\Anaconda\\envs\\NLP_2022\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///d%3A/Programmi/Anaconda/envs/NLP_2022/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(LSTM_fixed, epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "455534d435f7de67bc0026f9ceba702b21954bd7ad83505586b95ef58f556ae5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('NLP_2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
